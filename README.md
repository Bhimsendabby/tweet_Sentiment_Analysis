# Twitter Sentiment Analysis with SVM and Word2Vec

## Overview

This project performs sentiment analysis on a large Twitter dataset containing over **400,000 tweets** categorized as Positive, Negative, or Neutral. The goal is to build a Support Vector Machine (SVM) model using Word Embeddings generated by the **Word2Vec Continuous Bag of Words (CBOW)** model to classify the sentiments accurately.

---

## Table of Contents

- [Dataset](#dataset)
- [Project Structure](#project-structure)
- [Preprocessing Steps](#preprocessing-steps)
- [Word Embedding](#word-embedding)
- [Model Training](#model-training)
- [Model Evaluation](#model-evaluation)
- [Model Tuning and Cross-Validation](#model-tuning-and-cross-validation)
- [Results](#results)
- [Conclusion](#conclusion)
- [How to Run](#how-to-run)
- [Dependencies](#dependencies)

---

## Dataset

The dataset consists of tweets labeled with their corresponding sentiments:

- **Positive**
- **Negative**
- **Neutral**

---

## Project Structure

```plaintext
â”œâ”€â”€ data
â”‚   â””â”€â”€ tweets.csv             # Dataset file (not included)
â”œâ”€â”€ notebooks
â”‚   â””â”€â”€ sentiment_analysis.ipynb  # Jupyter Notebook with code
â”œâ”€â”€ models
â”‚   â””â”€â”€ svm_model.pkl          # Saved SVM model (if applicable)
â”œâ”€â”€ README.md                  # Project documentation
â””â”€â”€ requirements.txt           # Python dependencies
```

## Preprocessing Steps

Data preprocessing is crucial for improving model performance. The following steps were executed in order:

### 1. Loading the Dataset

- Imported the dataset using `pandas`.
- Checked for data consistency and explored basic statistics.

### 2. Removing URLs

- Used regular expressions to identify and remove URLs from tweets.
- **Example**:  
  - Original: `Check out this link http://example.com`  
  - Processed: `Check out this link`

### 3. Removing Slangs

- Created a slang dictionary to map slangs to their formal meanings.
- Replaced slangs in tweets using the dictionary.
- **Example**:  
  - Original: `lol`  
  - Processed: `laugh out loud`

### 4. Removing Emojis

- Utilized the `emoji` library to detect and remove emojis.
- **Example**:  
  - Original: `I'm happy ðŸ˜Š`  
  - Processed: `I'm happy`

### 5. Removing Punctuations

- Removed punctuation marks using string translation.
- **Example**:  
  - Original: `Hello!!!`  
  - Processed: `Hello`

### 6. Removing Stop Words

- Used NLTK's list of stop words to eliminate common words that do not contribute to sentiment.
- **Example**:  
  - Original: `This is a sample tweet`  
  - Processed: `This sample tweet`

### 7. Performing Lemmatization

- Applied NLTK's `WordNetLemmatizer` to reduce words to their base forms.
- **Example**:  
  - Original: `running`  
  - Processed: `run`

### 8. Tokenization

- Split the cleaned tweets into individual words (tokens).
- Prepared the data for word embedding.

