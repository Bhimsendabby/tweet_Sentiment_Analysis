# Twitter Sentiment Analysis with SVM and Word2Vec

## Overview

This project performs sentiment analysis on a large Twitter dataset containing over **400,000 tweets** categorized as Positive, Negative, or Neutral. The goal is to build a Support Vector Machine (SVM) model using Word Embeddings generated by the **Word2Vec Continuous Bag of Words (CBOW)** model to classify the sentiments accurately.

---

## Table of Contents

- [Dataset](#dataset)
- [Project Structure](#project-structure)
- [Preprocessing Steps](#preprocessing-steps)
- [Word Embedding](#word-embedding)
- [Model Training](#model-training)
- [Model Evaluation](#model-evaluation)
- [Model Tuning and Cross-Validation](#model-tuning-and-cross-validation)
- [Results](#results)
- [Conclusion](#conclusion)
- [How to Run](#how-to-run)
- [Dependencies](#dependencies)

---

## Dataset

The dataset consists of tweets labeled with their corresponding sentiments:

- **Positive**
- **Negative**
- **Neutral**

---

## Project Structure

```plaintext
â”œâ”€â”€ data
â”‚   â””â”€â”€ tweets.csv             # Dataset file (not included)
â”œâ”€â”€ notebooks
â”‚   â””â”€â”€ sentiment_analysis.ipynb  # Jupyter Notebook with code
â”œâ”€â”€ models
â”‚   â””â”€â”€ svm_model.pkl          # Saved SVM model (if applicable)
â”œâ”€â”€ README.md                  # Project documentation
â””â”€â”€ requirements.txt           # Python dependencies
```

## Preprocessing Steps

Data preprocessing is crucial for improving model performance. The following steps were executed in order:

### 1. Loading the Dataset

- Imported the dataset using `pandas`.
- Checked for data consistency and explored basic statistics.

### 2. Removing URLs

- Used regular expressions to identify and remove URLs from tweets.
- **Example**:  
  - Original: `Check out this link http://example.com`  
  - Processed: `Check out this link`

### 3. Removing Slangs

- Created a slang dictionary to map slangs to their formal meanings.
- Replaced slangs in tweets using the dictionary.
- **Example**:  
  - Original: `lol`  
  - Processed: `laugh out loud`

### 4. Removing Emojis

- Utilized the `emoji` library to detect and remove emojis.
- **Example**:  
  - Original: `I'm happy ðŸ˜Š`  
  - Processed: `I'm happy`

### 5. Removing Punctuations

- Removed punctuation marks using string translation.
- **Example**:  
  - Original: `Hello!!!`  
  - Processed: `Hello`

### 6. Removing Stop Words

- Used NLTK's list of stop words to eliminate common words that do not contribute to sentiment.
- **Example**:  
  - Original: `This is a sample tweet`  
  - Processed: `This sample tweet`

### 7. Performing Lemmatization

- Applied NLTK's `WordNetLemmatizer` to reduce words to their base forms.
- **Example**:  
  - Original: `running`  
  - Processed: `run`

### 8. Tokenization

- Split the cleaned tweets into individual words (tokens).
- Prepared the data for word embedding.

## Word Embedding

### Vectorizing Tokens using Word2Vec CBOW Model

- Trained a Word2Vec model with the following parameters:
  - **Architecture**: Continuous Bag of Words (CBOW)
  - **Vector Size**: 100 features
  - **Window Size**: Adjusted based on experimentation
- Converted each tweet into a numerical vector by averaging the Word2Vec embeddings of its tokens.

---

## Handling Null Values

### Null Value Handling

- Checked for any null values in the dataset after preprocessing.
- Handled null values by:
  - Removing rows with null values.
  - Optionally, imputing with appropriate methods if necessary.

---

## Splitting the Dataset

### Train-Test Split

- Divided the dataset into training and testing sets using an **80-20 split**.
- Ensured that the split is stratified to maintain the proportion of sentiment classes.

---

## Label Encoding

### Encoding Sentiment Labels

- Used scikit-learn's `LabelEncoder` to convert sentiment labels into numerical format:
  - **Positive**: `2`
  - **Neutral**: `1`
  - **Negative**: `0`

---

## Model Training

### Training the SVM Model

- Selected **SVM** as the classification algorithm due to its effectiveness in high-dimensional spaces.
- Used scikit-learn's `SVC` class with the following parameters:
  - **Kernel**: Linear (experimented with RBF as well)
  - **Regularization parameter C**: Tuned during model tuning
- Trained the model using the training dataset.

---

## Model Evaluation

### Calculating AUC Score and ROC Curve

- Predicted probabilities using `decision_function` or `predict_proba`.
- Calculated the **Area Under the Curve (AUC)** score.
- Plotted the **Receiver Operating Characteristic (ROC)** curve for each class.

### Calculating Training and Testing Accuracy

- Evaluated the model's accuracy on both the training and testing datasets.
- Used `accuracy_score` from scikit-learn.

### Calculating F1 Score

- Computed the **F1 score** to balance precision and recall.
- Calculated for each class using `f1_score` with `average='weighted'`.

### Confusion Matrix

- Generated a **confusion matrix** using `confusion_matrix`.
- Visualized it using a **heatmap** for better interpretation.

---

## Model Tuning and Cross-Validation

### Model Tuning

- Performed hyperparameter tuning using **Grid Search** or **Random Search**.
- Tuned the following parameters:
  - **Regularization parameter C**
  - **Kernel type** (Linear, RBF)
  - **Gamma** (for RBF kernel)
- Selected the model with the best cross-validation score.

### Cross-Validation using K-Fold

- Implemented **K-Fold Cross-Validation** with `K=5`.
- Ensured the model's performance is consistent across different data folds.
- Used `cross_val_score` for evaluation.

## Results

- **Training Accuracy**: 95% (example)
- **Testing Accuracy**: 93% (example)
- **AUC Score**: 0.96 (example)
- **F1 Score**: 0.93 (example)

### Confusion Matrix:

|                       | Predicted Negative | Predicted Neutral | Predicted Positive |
|-----------------------|--------------------|-------------------|--------------------|
| **Actual Negative**    | TN                 | FP                | FP                 |
| **Actual Neutral**     | FN                 | TN                | FP                 |
| **Actual Positive**    | FN                 | FP                | TP                 |

---

## Conclusion

- The SVM model, combined with Word2Vec embeddings, effectively classifies tweet sentiments.
- Preprocessing steps significantly improved the model's performance by cleaning the data.
- Model tuning and cross-validation ensured the robustness and generalizability of the model.

### Future improvements could include:

- Testing with different embedding methods like **GloVe** or **BERT**.
- Exploring other classification algorithms like **Random Forest** or **Neural Networks**.

---

## How to Run

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/twitter-sentiment-analysis.git
cd twitter-sentiment-analysis
```
